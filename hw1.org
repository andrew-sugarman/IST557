#+title: IST557 Homework 1
#+author: Andrew Sugarman
#+date: today

* Problem 1: Expected value of a sum of normal random variables

We are given: \(y \sim \mathcal{N}(\mu, \sigma^2) \) and \( y \in \mathbb{R} \). To solve the expectation \( E[y_{1}+y_{2}+y_{3}+ ... y_{N}] \) given \( N \in \mathbb{R} \), we can use the linearity of expectation:

$\\ E[y_{1}+y_{2}+y_{3}+ ... y_{N}] = E[y_{1}] + E[y_{2}] + E[y_{3}] + ... E[y_{N}] \\$

Since y is a normally distributed random variable with mean \mu, this expectation solves to be: \(N*\mu \).

* Problem 2: Conditional and Marginal Distribution of a Gaussian Heirarchical Time series

Refs:
https://www.otexts.com/fpp2/hts.html
https://en.wikipedia.org/wiki/Autoregressive_model

1. The conditional distribution is normally distributed. This hierarchical time series is autoregressive, and the y_0 instance is distributed normal.
2. This time series depends only on the value of the immediately prior condition (y_{t-1}).

* Problem 3: Gradient Descent Optimizer
Steps to write grad descent optimizer for pdf of students t distribution

- to effectively write an optimizer, we need to define a convex function
- with a convex function, we can pursue a gradient of 0
- the function given is the pdf of the student's t distribution, which maps from real space to probability space (0-1)
- to make the function convex and monotonic we can take the negative log likelihood of it
- taking the logarithm of this function simplifies it so that the first term separates out to a constant and you are left with: \( log(studt) = const + ((v+1)/2) * log(1+((b^2)/v)) \)
- next, the gradient must be calculated by calculating the partial derivative with respect to b, followed by calculating the partial derivative with respect to v (constant)
