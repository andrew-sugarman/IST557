#+title: IST557 Homework 1
#+author: Andrew Sugarman
#+date: \today
#+LATEX_HEADER: \usepackage{amsmath}

* Problem 1: Expected value of a sum of normal random variables

We are given: \(y \sim \mathcal{N}(\mu, \sigma^2) \) and \( y \in \mathbb{R} \). To solve the expectation \( E[y_{1}+y_{2}+y_{3}+ ... y_{N}] \) given \( N \in \mathbb{R} \), we can use the linearity of expectation:

$\\ E[y_{1}+y_{2}+y_{3}+ ... y_{N}] = E[y_{1}] + E[y_{2}] + E[y_{3}] + ... E[y_{N}] \\$

Since y is a normally distributed random variable with mean \mu, this expectation solves to be: \(N*\mu \).

* Problem 2: Conditional and Marginal Distribution of a Gaussian Heirarchical Time series

Refs:
https://www.otexts.com/fpp2/hts.html
https://en.wikipedia.org/wiki/Autoregressive_model

1. The conditional distribution is normally distributed. This hierarchical time series is autoregressive, and the y_0 instance is distributed normal.
2. This time series depends only on the value of the immediately prior condition (y_{t-1}).

* Problem 3: Gradient Descent Optimizer
Steps to write grad descent optimizer for $f(b)$:

\begin{equation}
f(b) = \frac{\Gamma((v+1) / 2)}{\sqrt(v\pi) \Gamma(v / 2)}(1 + (b^2/v))^{-(v+1)/2}
\end{equation}

With $v>0$

- to effectively write an optimizer, we need to define a convex function
- with a convex function, we can pursue the minimum gradient
- the function given is the pdf of the student's t distribution, which maps from real space to probability space (0-1)
- to make the function convex and monotonic we can take its negative logarithm:

  \begin{multiline*}

  -log(f(b)) = -[\log( \frac{\Gamma((v+1) / 2)}{\sqrt(v\pi) \Gamma(v / 2)} ) + \log((1 + (b^2/v))^{ -(v+1)/2 }]\\

  -log(f(b)) = -[\log(\Gamma((v+1) / 2)) - \log(\sqrt(v\pi) \Gamma(v / 2)) + (-(v+1)/2) \log((1 + (b^2/v))]

  \end{multiline*}

- This allows us to split from this equation a term dependent on v:

  x = \( \log(\Gamma(\frac{v+1}{2})) - (1/2)\log(v\pi) - \log(\Gamma(v/2)) \) allowing us to write the term dependent on b and constant v as y = \((-(v+1)/2) \log((1 + (b^2/v)\)

- Thus: \(-log(f(b) = -(a + b)\)
- Next we set up gradient descent by calculating the partial derivative with respect to b, followed by calculating the partial derivative with respect to v (constant)
- The partial derivative of $-log(f(b))$ with respect to b is:
\begin{equation}
  \frac{\partial f(b)}{\partial b} = \frac{b(1+v)}{v+b^2}
\end{equation}

- The partial derivative of the gamma function is eluding me at this time but knowing that v=3 we can solve this problem:
